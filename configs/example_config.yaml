experiment_name: test # Name of the experiment, used for saving files

model_name: zeppelin_model # Only option in this repository (for (MSMT-)CSD see https://github.com/tomhend/MSMT-CSD_INR)
output_calculator: zeppelin_model # Choose from: standard_model, standard_model_num, grad_cor_num
loss_function_name: mselossunregl2 # Choose from: mselossavgl2, lossavgl1, ricianlogloss (requires s_noise parameter)
# s_noise: 0 # Only used for ricianlogloss, can be removed for mse loss

gaussian_encoding: True # Deprecated, keep on true
fod_rescale: True # Deprecated, keep on true
dataset_name: zeppelin # Should correspond with output_calculator: zeppelin
bval_delta: 0.1 # define margins on b-values

scale_data: norm # Leave on norm for now (normalization based on 95th percentile of b0)
rescale_output: True # Rescale the s0 and dwi images to original magnitudes

clip_gradients: 0 # 0 is no clipping
lr_scheduler: True # Use of learning rate scheduler, settings hard-coded in main.py

width: # Width of input image
height:  # Height of input image
depth:  # Depth of input image

log_freq: 0 # Passed to the trainer, useful if you want to implement your own logging system.

paths:
  nifti: data/recon_signal_lmax2_Gaussian_SNR50.nii.gz # input nifti
  mask: data/wm_mask_011.nii.gz # White matter mask
  fsl_bvecs: data/bvecs_grad_prisma_new.bvec # bvec file, expected in x,y,z per line
  fsl_bvals: data/bvecs_grad_prisma_new.bval # bval file, expected in one value per line
  fsl_bdelta: data/bvecs_grad_prisma_new.bdelta # bdelta file, expected in one value per line
  # c_bvecs: .npy with corrected bvecs for gradient correction, expected shape (W, H, D, N_dir, 3)
  # c_bvals: .npy with corrected bvals for gradient correction, expected shape (W, H, D, N_dir)
  # c_bdelta: .npy with corrected bdeltas for gradient correction, expected shape (W, H, D, N_dir)
  output: outputs/ # output folder, is created in the process

train_cfg:
  nr_fiber_directions: 50 # SH order of the output fod
  lpos: 5000 # Number of positional encodings (/2)
  n_layers: 4 # Number of layers in the network
  hidden_dim: 2048 # Size of hidden dims
  lr: 0.00001 # Learning rate
  epochs: 100 # Number of epochs
  batch_size: 100 # Batch size
  lambda: 5 # Strength of non-negativity constraint on FODs, 0 for no non-negativity constraint
  sigma: 3.5 # Variance of the normal distribution where the encoding frequencies are sampled from
